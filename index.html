<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Claudio Greco</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü¶Ñ</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="navbar-brand social">
          <a href="mailto:%63%6C%61%75%64%69%6F.%67%72%65%63%6F@%75%6E%69%74%6E.%69%74"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=https://scholar.google.it/citations?hl=it&user=AX5jab0AAAAJ&view_op=list_works&sortby=pubdate" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/claudiogreco" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>

<a href="https://twitter.com/cld_greco" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>









        </div>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
          <a class="nav-link" href="https://scholar.google.com/citations?user=https://scholar.google.it/citations?hl=it&user=AX5jab0AAAAJ&view_op=list_works&sortby=pubdate">publications</a>
          </li>
          
          <li class="nav-item ">
          <a class="nav-link" href="/assets/documents/Claudio_Greco_CV.pdf">curriculum vitae</a>
          </li>
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     Claudio Greco
    </h1>
     <p class="desc"></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg">
      
      
    </div>
    

    <div class="clearfix">
      <p style="text-align: justify;">
I am a 4th year Ph.D student in Cognitive and Brain Sciences at the <a href="https://www.cimec.unitn.it/en">Center for Mind/Brain Sciences</a> (<a href="https://www.unitn.it/en">University of Trento</a>) working in the <a href="https://www.cimec.unitn.it/en/257/language-and-vision-lavi">Language and Vision Research Group</a> under the supervision of <a href="http://disi.unitn.it/~bernardi/">Raffaella Bernardi</a>. Before enrolling in my Ph.D program, I obtained M.Sc. and B.Sc. degrees in Computer Science at the <a href="https://www.uniba.it/english-version">University of Bari</a>, where I worked on social network analysis and (conversational) recommender systems, respectively.
<br />
<br />
My work lies at the intersection between <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a>, <a href="https://en.wikipedia.org/wiki/Computer_vision">Computer Vision</a>, and <a href="https://en.wikipedia.org/wiki/Cognitive_science">Cognitive Science</a>. My research focuses on training deep neural networks from several modalities such as language and vision. I am especially interested in understanding how grounded conversational agents can learn new tasks as quickly as possible reusing the previously-learned skills without forgetting them.
<br />
</p>

    </div>

    
      <div class="news">
  <h2>highlights</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">May 14, 2021</th>
          <td>
            
              I am presenting the paper <a href="https://www.aclweb.org/anthology/P19-1350.pdf">Psycholinguistics Meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering</a> (<a href="https://acl2019.org/EN/index.xhtml.html">ACL 2019</a>) at the <a href="https://www.continualai.org/reading_group/">ContinualAI Reading Group</a>!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Apr 16, 2021</th>
          <td>
            
              The paper <em>‚ÄúYes‚Äù and ‚ÄúNo‚Äù: Visually Grounded Polar Answers</em> has been accepted at <a href="https://vigilworkshop.github.io/">ViGIL 2021</a>!

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP 2020</abbr>
    
  
  </div>

  <div id="pezzelle2020different" class="col-sm-8">
    
      <div class="title">Be Different to Be Better! A Benchmark to Leverage the Complementarity of Language and Vision</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Pezzelle, Sandro,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Greco, Claudio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gandolfi, Greta,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gualdoni, Eleonora,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Bernardi, Raffaella
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.248.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper introduces BD2BB, a novel language and vision benchmark that requires multimodal models combine complementary information from the two modalities. Recently, impressive progress has been made to develop universal multimodal encoders suitable for virtually any language and vision tasks. However, current approaches often require them to combine redundant information provided by language and vision. Inspired by real-life communicative contexts, we propose a novel task where either modality is necessary but not sufficient to make a correct prediction. To do so, we first build a dataset of images and corresponding sentences provided by human participants. Second, we evaluate state-of-the-art models and compare their performance against human speakers. We show that, while the task is relatively easy for humans, best-performing models struggle to achieve similar results.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SpLU 2020</abbr>
    
  
  </div>

  <div id="testoni2020they" class="col-sm-8">
    
      <div class="title">They are not all alike: answering different spatial questions requires different grounding strategies</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Testoni, Alberto,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Greco, Claudio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Bianchi, Tobias,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Mazuecos, Mauricio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Marcante, Agata,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Benotti, Luciana,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Bernardi, Raffaella
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the Third International Workshop on Spatial Language Understanding</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.aclweb.org/anthology/2020.splu-1.4.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we study the grounding skills required to answer spatial questions asked by humans while playing the GuessWhat?! game. We propose a classification for spatial questions dividing them into absolute, relational, and group questions. We build a new answerer model based on the LXMERT multimodal transformer and we compare a baseline with and without visual features of the scene. We are interested in studying how the attention mechanisms of LXMERT are used to answer spatial questions since they require putting attention on more than one region simultaneously and spotting the relation holding among them. We show that our proposed model outperforms the baseline by a large extent (9.70% on spatial questions and 6.27% overall). By analyzing LXMERT errors and its attention mechanisms, we find that our classification helps to gain a better understanding of the skills required to answer different spatial questions.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NL4AI 2020</abbr>
    
  
  </div>

  <div id="DBLP:conf/aiia/0002TB19" class="col-sm-8">
    
      <div class="title">Which Turn do Neural Models Exploit the Most to Solve GuessWhat? Diving
               into the Dialogue History Encoding in Transformers and LSTMs</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Greco, Claudio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Testoni, Alberto,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Bernardi, Raffaella
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 4th Workshop on Natural Language for Artificial
               Intelligence (NL4AI 2020) co-located with the 19th International
               Conference of the Italian Association for Artificial Intelligence
               (AI*IA 2020), Anywhere, November 25th-27th, 2020</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
    
    
    
      
      <a href="http://ceur-ws.org/Vol-2735/paper31.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL 2019</abbr>
    
  
  </div>

  <div id="greco2019psycholinguistics" class="col-sm-8">
    
      <div class="title">Psycholinguistics Meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Greco, Claudio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Plank, Barbara,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Fern√°ndez, Raquel,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Bernardi, Raffaella
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.aclweb.org/anthology/P19-1350.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We study the issue of catastrophic forgetting in the context of neural multimodal approaches to Visual Question Answering (VQA). Motivated by evidence from psycholinguistics, we devise a set of linguistically-informed VQA tasks, which differ by the types of questions involved (Wh-questions and polar questions). We test what impact task difficulty has on continual learning, and whether the order in which a child acquires question types facilitates computational models. Our results show that dramatic forgetting is at play and that task difficulty and order matter. Two well-known current continual learning methods mitigate the problem only to a limiting degree.</p>
    </div>
    
  </div>
</div>
</li></ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%63%6C%61%75%64%69%6F.%67%72%65%63%6F@%75%6E%69%74%6E.%69%74"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=https://scholar.google.it/citations?hl=it&user=AX5jab0AAAAJ&view_op=list_works&sortby=pubdate" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/claudiogreco" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>

<a href="https://twitter.com/cld_greco" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>









      </div>
      <div class="contact-note"></div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container" align="center">
    &copy; Copyright 2021 Claudio  Greco.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
