<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Claudio Greco</title>
<meta name="description" content="A beautiful, simple, clean, and responsive Jekyll theme for academics">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü¶Ñ</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="navbar-brand social">
          <a href="mailto:%63%6C%61%75%64%69%6F.%67%72%65%63%6F@%75%6E%69%74%6E.%69%74"><i class="fas fa-envelope"></i></a>




<a href="https://github.com/claudiogreco" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>

<a href="https://twitter.com/cld_greco" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>









        </div>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
          <a class="nav-link" href="https://scholar.google.com/citations?user=https://scholar.google.it/citations?hl=it&user=AX5jab0AAAAJ&view_op=list_works&sortby=pubdate">publications</a>
          </li>
          
          <li class="nav-item ">
          <a class="nav-link" href="/assets/documents/Claudio_Greco_CV.pdf">curriculum vitae</a>
          </li>
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     Claudio Greco
    </h1>
     <p class="desc"><a href="https://www.cimec.unitn.it/en">CIMeC</a>, <a href="https://www.unitn.it/en">University of Trento</a>.</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg">
      
      
    </div>
    

    <div class="clearfix">
      <p style="text-align: justify;">
I am a 4th year Ph.D student in Cognitive and Brain Sciences at the <a href="https://www.cimec.unitn.it/en">Center for Mind/Brain Sciences</a> (<a href="https://www.unitn.it/en">University of Trento</a>) working in the <a href="https://www.cimec.unitn.it/en/257/language-and-vision-lavi">Language and Vision Research Group</a> under the supervision of <a href="http://disi.unitn.it/~bernardi/">Raffaella Bernardi</a>. Before enrolling in my Ph.D program, I obtained B.Sc. and M.Sc. degrees in Computer Science at the <a href="https://www.uniba.it/english-version">University of Bari</a>, where I worked on social network analysis and (conversational) recommender systems, respectively.
<br />
<br />
My work lies at the intersection between <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a>, <a href="https://en.wikipedia.org/wiki/Computer_vision">Computer Vision</a>, and <a href="https://en.wikipedia.org/wiki/Cognitive_science">Cognitive Science</a>. My research focuses on training deep neural networks from several modalities such as language and vision. I am especially interested in understanding how grounded conversational agents can learn new tasks as quickly as possible reusing the previously-learned skills without forgetting them.
<br />
</p>

    </div>

    
      <div class="news">
  <h2>highlights</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">May 14, 2021</th>
          <td>
            
              I am presenting the paper <a href="https://www.aclweb.org/anthology/P19-1350.pdf">Psycholinguistics Meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering</a> (<a href="https://acl2019.org/EN/index.xhtml.html">ACL 2019</a>) at the <a href="https://www.continualai.org/reading_group/">ContinualAI Reading Group</a>!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Apr 16, 2021</th>
          <td>
            
              The paper <em>‚ÄúYes‚Äù and ‚ÄúNo‚Äù: Visually Grounded Polar Answers</em> has been accepted at <a href="https://vigilworkshop.github.io/">ViGIL 2021</a>!

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP 2020</abbr>
    
  
  </div>

  <div id="pezzelle2020different" class="col-sm-8">
    
      <div class="title">Be Different to Be Better! A Benchmark to Leverage the Complementarity of Language and Vision</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Pezzelle, Sandro,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Greco, Claudio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gandolfi, Greta,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gualdoni, Eleonora,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Bernardi, Raffaella
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.248.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/sandropezzelle/bd2bb" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper introduces BD2BB, a novel language and vision benchmark that requires multimodal models combine complementary information from the two modalities. Recently, impressive progress has been made to develop universal multimodal encoders suitable for virtually any language and vision tasks. However, current approaches often require them to combine redundant information provided by language and vision. Inspired by real-life communicative contexts, we propose a novel task where either modality is necessary but not sufficient to make a correct prediction. To do so, we first build a dataset of images and corresponding sentences provided by human participants. Second, we evaluate state-of-the-art models and compare their performance against human speakers. We show that, while the task is relatively easy for humans, best-performing models struggle to achieve similar results.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SpLU 2020</abbr>
    
  
  </div>

  <div id="testoni2020they" class="col-sm-8">
    
      <div class="title">They are not all alike: answering different spatial questions requires different grounding strategies</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Testoni, Alberto,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Greco, Claudio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Bianchi, Tobias,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Mazuecos, Mauricio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Marcante, Agata,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Benotti, Luciana,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Bernardi, Raffaella
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the Third International Workshop on Spatial Language Understanding</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.aclweb.org/anthology/2020.splu-1.4.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/albertotestoni/unitn_unc_splu2020" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we study the grounding skills required to answer spatial questions asked by humans while playing the GuessWhat?! game. We propose a classification for spatial questions dividing them into absolute, relational, and group questions. We build a new answerer model based on the LXMERT multimodal transformer and we compare a baseline with and without visual features of the scene. We are interested in studying how the attention mechanisms of LXMERT are used to answer spatial questions since they require putting attention on more than one region simultaneously and spotting the relation holding among them. We show that our proposed model outperforms the baseline by a large extent (9.70% on spatial questions and 6.27% overall). By analyzing LXMERT errors and its attention mechanisms, we find that our classification helps to gain a better understanding of the skills required to answer different spatial questions.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AIxIA 2020</abbr>
    
  
  </div>

  <div id="DBLP:conf/aiia/0002TB19" class="col-sm-8">
    
      <div class="title">Grounding Dialogue History: Strengths and Weaknesses of Pre-Trained Transformers</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Greco, Claudio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Testoni, Alberto,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Bernardi, Raffaella
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In XIXth International Conference of the Italian Association for Artificial Intelligence, Virtual Event, November 24-27, 2020, Revised and Selected papers, Vol 12414 of LNAI, Springer Nature Switzerland AG</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
      <a href="https://github.com/claudiogreco/aixia2021" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We focus on visually grounded dialogue history encoding. We show that GuessWhat?! can be used as a ‚Äúdiagnostic‚Äù dataset to understand whether State-of-the-Art encoders manage to capture salient information in the dialogue history. We compare models across several dimensions: the architecture (Recurrent Neural Networks vs. Transformers), the input modalities (only language vs. language and vision), and the model background knowledge (trained from scratch vs. pre-trained and then fine-tuned on the downstream task). We show that pre-trained Transformers, RoBERTa and LXMERT, are able to identify the most salient information independently of the order in which the dialogue history is processed. Moreover, we find that RoBERTa handles the dialogue structure to some extent; instead LXMERT can effectively ground short dialogues, but it fails in processing longer dialogues having a more complex structure.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL 2019</abbr>
    
  
  </div>

  <div id="greco2019psycholinguistics" class="col-sm-8">
    
      <div class="title">Psycholinguistics Meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Greco, Claudio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Plank, Barbara,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Fern√°ndez, Raquel,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Bernardi, Raffaella
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.aclweb.org/anthology/P19-1350.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We study the issue of catastrophic forgetting in the context of neural multimodal approaches to Visual Question Answering (VQA). Motivated by evidence from psycholinguistics, we devise a set of linguistically-informed VQA tasks, which differ by the types of questions involved (Wh-questions and polar questions). We test what impact task difficulty has on continual learning, and whether the order in which a child acquires question types facilitates computational models. Our results show that dramatic forgetting is at play and that task difficulty and order matter. Two well-known current continual learning methods mitigate the problem only to a limiting degree.</p>
    </div>
    
  </div>
</div>
</li></ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%63%6C%61%75%64%69%6F.%67%72%65%63%6F@%75%6E%69%74%6E.%69%74"><i class="fas fa-envelope"></i></a>




<a href="https://github.com/claudiogreco" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>

<a href="https://twitter.com/cld_greco" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>









      </div>
      <div class="contact-note"></div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container" align="center">
    &copy; Copyright 2021 Claudio  Greco.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
