---
---

@string{aps = {American Physical Society,}}

@inproceedings{pezzelle2020different,
  abbr={EMNLP 2020},
  title={Be Different to Be Better! A Benchmark to Leverage the Complementarity of Language and Vision},
  author={Pezzelle, Sandro and Greco, Claudio and Gandolfi, Greta and Gualdoni, Eleonora and Bernardi, Raffaella},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},
  pages={2751--2767},
  year={2020},
  abstract={This paper introduces BD2BB, a novel language and vision benchmark that requires multimodal models combine complementary information from the two modalities. Recently, impressive progress has been made to develop universal multimodal encoders suitable for virtually any language and vision tasks. However, current approaches often require them to combine redundant information provided by language and vision. Inspired by real-life communicative contexts, we propose a novel task where either modality is necessary but not sufficient to make a correct prediction. To do so, we first build a dataset of images and corresponding sentences provided by human participants. Second, we evaluate state-of-the-art models and compare their performance against human speakers. We show that, while the task is relatively easy for humans, best-performing models struggle to achieve similar results.},
  pdf={https://www.aclweb.org/anthology/2020.findings-emnlp.248.pdf},
  selected={true}
}

@inproceedings{testoni2020they,
  abbr={SpLU 2020},
  title={They are not all alike: answering different spatial questions requires different grounding strategies},
  author={Testoni, Alberto and Greco, Claudio and Bianchi, Tobias and Mazuecos, Mauricio and Marcante, Agata and Benotti, Luciana and Bernardi, Raffaella},
  booktitle={Proceedings of the Third International Workshop on Spatial Language Understanding},
  pages={29--38},
  year={2020},
  abstract={In this paper, we study the grounding skills required to answer spatial questions asked by humans while playing the GuessWhat?! game. We propose a classification for spatial questions dividing them into absolute, relational, and group questions. We build a new answerer model based on the LXMERT multimodal transformer and we compare a baseline with and without visual features of the scene. We are interested in studying how the attention mechanisms of LXMERT are used to answer spatial questions since they require putting attention on more than one region simultaneously and spotting the relation holding among them. We show that our proposed model outperforms the baseline by a large extent (9.70% on spatial questions and 6.27% overall). By analyzing LXMERT errors and its attention mechanisms, we find that our classification helps to gain a better understanding of the skills required to answer different spatial questions.},
  pdf={https://www.aclweb.org/anthology/2020.splu-1.4.pdf},
  selected={true}
}

@inproceedings{DBLP:conf/aiia/0002TB19,
  abbr={NL4AI 2020},
  author    = {Claudio Greco and
               Alberto Testoni and
               Raffaella Bernardi},
  editor    = {Pierpaolo Basile and
               Valerio Basile and
               Danilo Croce and
               Elena Cabrio},
  title     = {Which Turn do Neural Models Exploit the Most to Solve GuessWhat? Diving
               into the Dialogue History Encoding in Transformers and LSTMs},
  booktitle = {Proceedings of the 4th Workshop on Natural Language for Artificial
               Intelligence {(NL4AI} 2020) co-located with the 19th International
               Conference of the Italian Association for Artificial Intelligence
               (AI*IA 2020), Anywhere, November 25th-27th, 2020},
  series    = {{CEUR} Workshop Proceedings},
  volume    = {2735},
  pages     = {29--43},
  publisher = {CEUR-WS.org},
  year      = {2020},
  pdf       = {http://ceur-ws.org/Vol-2735/paper31.pdf},
  timestamp = {Thu, 03 Dec 2020 16:09:56 +0100},
  biburl    = {https://dblp.org/rec/conf/aiia/0002TB19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  selected={true}
}

@inproceedings{greco2019psycholinguistics,
  abbr={ACL 2019},
  title={Psycholinguistics Meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering},
  author={Greco, Claudio and Plank, Barbara and Fern{\'a}ndez, Raquel and Bernardi, Raffaella},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3601--3605},
  year={2019},
  abstract={We study the issue of catastrophic forgetting in the context of neural multimodal approaches to Visual Question Answering (VQA). Motivated by evidence from psycholinguistics, we devise a set of linguistically-informed VQA tasks, which differ by the types of questions involved (Wh-questions and polar questions). We test what impact task difficulty has on continual learning, and whether the order in which a child acquires question types facilitates computational models. Our results show that dramatic forgetting is at play and that task difficulty and order matter. Two well-known current continual learning methods mitigate the problem only to a limiting degree.},
  pdf={https://www.aclweb.org/anthology/P19-1350.pdf},
  selected={true}
}
