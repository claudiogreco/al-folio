---
---

@string{aps = {American Physical Society,}}

@inproceedings{pezzelle2020different,
  abbr={EMNLP 2020},
  title={Be Different to Be Better! A Benchmark to Leverage the Complementarity of Language and Vision},
  author={Pezzelle, Sandro and Greco, Claudio and Gandolfi, Greta and Gualdoni, Eleonora and Bernardi, Raffaella},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},
  pages={2751--2767},
  year={2020},
  abstract={This paper introduces BD2BB, a novel language and vision benchmark that requires multimodal models combine complementary information from the two modalities. Recently, impressive progress has been made to develop universal multimodal encoders suitable for virtually any language and vision tasks. However, current approaches often require them to combine redundant information provided by language and vision. Inspired by real-life communicative contexts, we propose a novel task where either modality is necessary but not sufficient to make a correct prediction. To do so, we first build a dataset of images and corresponding sentences provided by human participants. Second, we evaluate state-of-the-art models and compare their performance against human speakers. We show that, while the task is relatively easy for humans, best-performing models struggle to achieve similar results.},
  pdf={https://www.aclweb.org/anthology/2020.findings-emnlp.248.pdf},
  code={https://github.com/sandropezzelle/bd2bb},
  selected={true}
}

@inproceedings{testoni2020they,
  abbr={SpLU 2020},
  title={They are not all alike: answering different spatial questions requires different grounding strategies},
  author={Testoni, Alberto and Greco, Claudio and Bianchi, Tobias and Mazuecos, Mauricio and Marcante, Agata and Benotti, Luciana and Bernardi, Raffaella},
  booktitle={Proceedings of the Third International Workshop on Spatial Language Understanding},
  pages={29--38},
  year={2020},
  abstract={In this paper, we study the grounding skills required to answer spatial questions asked by humans while playing the GuessWhat?! game. We propose a classification for spatial questions dividing them into absolute, relational, and group questions. We build a new answerer model based on the LXMERT multimodal transformer and we compare a baseline with and without visual features of the scene. We are interested in studying how the attention mechanisms of LXMERT are used to answer spatial questions since they require putting attention on more than one region simultaneously and spotting the relation holding among them. We show that our proposed model outperforms the baseline by a large extent (9.70% on spatial questions and 6.27% overall). By analyzing LXMERT errors and its attention mechanisms, we find that our classification helps to gain a better understanding of the skills required to answer different spatial questions.},
  pdf={https://www.aclweb.org/anthology/2020.splu-1.4.pdf},
  code={https://github.com/albertotestoni/unitn_unc_splu2020},
  selected={true}
}

@inproceedings{DBLP:conf/aiia/0002TB19,
  abbr={AIxIA 2020},
  author    = {Claudio Greco and
               Alberto Testoni and
               Raffaella Bernardi},
  editor    = {Matteo Baldoni and
               Stefania bandini},
  title     = {Grounding Dialogue History: Strengths and Weaknesses of Pre-Trained Transformers},
  booktitle = {XIXth International Conference of the Italian Association for Artificial Intelligence, Virtual Event, November 24-27, 2020, Revised and Selected papers},
  series    = {AIxIA 2020: Advances in Artificial Intelligence},
  volume    = {Volume 12414 of LNAI},
  publisher = {Springer Nature Switzerland AG, 2021},
  year      = {2020},
  abstract  = {We focus on visually grounded dialogue history encoding. We show that GuessWhat?! can be used as a “diagnostic” dataset to understand whether State-of-the-Art encoders manage to capture salient information in the dialogue history. We compare models across several dimensions: the architecture (Recurrent Neural Networks vs. Transformers), the input modalities (only language vs. language and vision), and the model background knowledge (trained from scratch vs. pre-trained and then fine-tuned on the downstream task). We show that pre-trained Transformers, RoBERTa and LXMERT, are able to identify the most salient information independently of the order in which the dialogue history is processed. Moreover, we find that RoBERTa handles the dialogue structure to some extent; instead LXMERT can effectively ground short dialogues, but it fails in processing longer dialogues having a more complex structure.},
  code={https://github.com/claudiogreco/aixia2021},
  selected={true}
}

@inproceedings{greco2019psycholinguistics,
  abbr={ACL 2019},
  title={Psycholinguistics Meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering},
  author={Greco, Claudio and Plank, Barbara and Fern{\'a}ndez, Raquel and Bernardi, Raffaella},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3601--3605},
  year={2019},
  abstract={We study the issue of catastrophic forgetting in the context of neural multimodal approaches to Visual Question Answering (VQA). Motivated by evidence from psycholinguistics, we devise a set of linguistically-informed VQA tasks, which differ by the types of questions involved (Wh-questions and polar questions). We test what impact task difficulty has on continual learning, and whether the order in which a child acquires question types facilitates computational models. Our results show that dramatic forgetting is at play and that task difficulty and order matter. Two well-known current continual learning methods mitigate the problem only to a limiting degree.},
  pdf={https://www.aclweb.org/anthology/P19-1350.pdf},
  selected={true}
}
